{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:38.740015377Z",
     "start_time": "2026-02-08T12:01:38.683602054Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.io import read_video\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import warnings\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:38.787120279Z",
     "start_time": "2026-02-08T12:01:38.740760866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppress torchvision video deprecation warning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchvision.io\")"
   ],
   "id": "bed8f0312ead052e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:38.837901724Z",
     "start_time": "2026-02-08T12:01:38.788488145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_CLIP = 16\n",
    "IMG_SIZE = 112  # R(2+1)D expects 112x112\n"
   ],
   "id": "f3396d1f1be74d29",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:38.887074912Z",
     "start_time": "2026-02-08T12:01:38.838601095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, frames_per_clip=16, transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve relative path from dataframe\n",
    "        # Assuming clip_path is like './dataset/Diving/v_Diving_g03_c01.avi'\n",
    "        # We need to construct the full absolute path or correct relative path.\n",
    "        # Since the script is running from the project root, './dataset/...' is correct.\n",
    "\n",
    "        video_path = self.df.iloc[idx]['clip_path']\n",
    "        label = int(self.df.iloc[idx]['encoded_label'])\n",
    "\n",
    "        # Determine actual file path\n",
    "        # If running from /home/yogendra/workspace/python/university/Video-Analytics-Assignment1/\n",
    "        # and clip_path is ./dataset/..., it works locally.\n",
    "\n",
    "        try:\n",
    "            # read_video returns (T, H, W, C) in [0, 255]\n",
    "            video, _, info = read_video(video_path, pts_unit='sec')\n",
    "            # Note: using default THWC format\n",
    "        except Exception as e:\n",
    "            # Handle read errors (e.g. corrupt video) by returning a zero tensor or skipping\n",
    "            print(f\"Error reading {video_path}: {e}\")\n",
    "            return torch.zeros((3, self.frames_per_clip, IMG_SIZE, IMG_SIZE)), label\n",
    "\n",
    "        # video is (T, H, W, C)\n",
    "        total_frames = video.shape[0]\n",
    "\n",
    "        # Temporal resampling\n",
    "        if total_frames >= self.frames_per_clip:\n",
    "            # Uniformly sample frames\n",
    "            indices = np.linspace(0, total_frames - 1, self.frames_per_clip).astype(int)\n",
    "            video = video[indices]\n",
    "        else:\n",
    "            # Loop video to fill frames\n",
    "            needed = self.frames_per_clip - total_frames\n",
    "            # Simple padding: repeat last frame or cycle\n",
    "            # Let's cycle\n",
    "            indices = np.resize(np.arange(total_frames), self.frames_per_clip)\n",
    "            video = video[indices]\n",
    "\n",
    "        # Current shape: (F, H, W, C)\n",
    "        # Permute to (C, F, H, W) for transforms/model\n",
    "        video = video.permute(3, 0, 1, 2)\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        video = video.float() / 255.0\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        return video, label\n"
   ],
   "id": "bee58f43f5e35750",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:38.938502086Z",
     "start_time": "2026-02-08T12:01:38.888313272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_transform():\n",
    "    # Helper to apply spatial transforms to (C, F, H, W)\n",
    "    # Note: torchvision.transforms usually expects (C, H, W).\n",
    "    # We can treat (F*C, H, W) or create custom wrapper.\n",
    "    # For simplicity and R(2+1)D:\n",
    "    # Resize and CenterCrop are standard.\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((128, 171)), # Standard kinetic resize\n",
    "        transforms.RandomCrop(IMG_SIZE),\n",
    "        transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "    ])\n",
    "\n",
    "def get_val_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((128, 171)),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "    ])\n"
   ],
   "id": "7dee0a9456ca71eb",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:39.007047558Z",
     "start_time": "2026-02-08T12:01:38.941510831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom Transform wrapper to handle video tensor (C, F, H, W)\n",
    "# Standard transforms work on (C, H, W). We can reshape to (C*F, H, W) -> transform -> (C, F, H, W)\n",
    "# OR manually apply to each frame.\n",
    "# Reshape approach is cleaner for spatial transforms that don't depend on frame idx (like Resize, Crop).\n",
    "# BUT RandomCrop needs to be same for all frames.\n",
    "# Using ReplayTransform or specialized video transforms is best.\n",
    "# For this assignment, let's implement a simple wrapper class.\n",
    "\n",
    "class VideoTransform:\n",
    "    def __init__(self, transform, is_train=True):\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x is (C, F, H, W)\n",
    "        C, F, H, W = x.shape\n",
    "        # Permute to (F, C, H, W) to apply transform frame by frame?\n",
    "        # No, that's slow and random transforms would be different per frame.\n",
    "        # We assume x is a tensor.\n",
    "\n",
    "        # If we use pytorch transforms that support batch, we can pass (F, C, H, W)\n",
    "        # But Resize/Crop usually work on (..., H, W).\n",
    "\n",
    "        # Let's reshape to (F, C, H, W) first\n",
    "        x = x.permute(1, 0, 2, 3) # (F, C, H, W)\n",
    "\n",
    "        # Standardize RandomCrop: use torch.seed or apply same params.\n",
    "        # However, simplistic approach for 'RandomCrop':\n",
    "        # Just use CenterCrop for validation, and for training if we accept slight jitter per frame (bad)\n",
    "        # OR use functional API.\n",
    "\n",
    "        # Better approach for this script:\n",
    "        # Resize generally.\n",
    "        x = transforms.functional.resize(x, (128, 171))\n",
    "\n",
    "        if self.is_train:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(x, output_size=(IMG_SIZE, IMG_SIZE))\n",
    "            x = transforms.functional.crop(x, i, j, h, w)\n",
    "            # Maybe Horizontal Flip\n",
    "            if torch.rand(1) < 0.5:\n",
    "                x = transforms.functional.hflip(x)\n",
    "        else:\n",
    "            x = transforms.functional.center_crop(x, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        # Normalize\n",
    "        # Normalize expect (C, H, W). We have (F, C, H, W).\n",
    "        # We can iterate or transpose.\n",
    "        mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.22803, 0.22145, 0.216989]).view(1, 3, 1, 1)\n",
    "\n",
    "        # Normalize\n",
    "        # Normalize expect (C, H, W) or (B, C, H, W). We have (F, C, H, W).\n",
    "        mean = torch.tensor([0.43216, 0.394666, 0.37645]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.22803, 0.22145, 0.216989]).view(1, 3, 1, 1)\n",
    "\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        # x is (F, C, H, W). Permute back to (C, F, H, W)\n",
    "        x = x.permute(1, 0, 2, 3) # (C, F, H, W)\n",
    "\n",
    "        return x\n"
   ],
   "id": "486268a80e062383",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:39.071864359Z",
     "start_time": "2026-02-08T12:01:39.010329297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n"
   ],
   "id": "87318c9067562a7d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:39.126710405Z",
     "start_time": "2026-02-08T12:01:39.073027256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return epoch_loss, epoch_acc, all_labels, all_preds\n"
   ],
   "id": "9a111c6aa17f9fdc",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:01:39.186591887Z",
     "start_time": "2026-02-08T12:01:39.128068690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(dry_run=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load Dataframes\n",
    "    train_df = pd.read_csv(\"./dataset/splits/train.csv\", index_col='index')\n",
    "    val_df = pd.read_csv(\"./dataset/splits/validation.csv\", index_col='index')\n",
    "    test_df = pd.read_csv(\"./dataset/splits/test.csv\", index_col='index')\n",
    "\n",
    "    if dry_run:\n",
    "        print(\"DRY RUN MODE: limiting dataset size\")\n",
    "        train_df = train_df.iloc[:20]\n",
    "        val_df = val_df.iloc[:10]\n",
    "        test_df = test_df.iloc[:10]\n",
    "        global NUM_EPOCHS\n",
    "        NUM_EPOCHS = 2\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = VideoDataset(train_df, root_dir='./', transform=VideoTransform(None, is_train=True))\n",
    "    val_dataset = VideoDataset(val_df, root_dir='./', transform=VideoTransform(None, is_train=False))\n",
    "    test_dataset = VideoDataset(test_df, root_dir='./', transform=VideoTransform(None, is_train=False))\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Model Setup\n",
    "    print(\"Initializing R(2+1)D model...\")\n",
    "    weights = torchvision.models.video.R2Plus1D_18_Weights.DEFAULT\n",
    "    model = torchvision.models.video.r2plus1d_18(weights=weights)\n",
    "\n",
    "    # Modify final layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, NUM_CLASSES)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimization\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # SGD with momentum as suggested, or Adam\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), \"best_model_r2plus1d.pth\")\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    # Load best model for testing\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Test Evaluation\n",
    "    print(\"\\nEvaluating on Test Set...\")\n",
    "    test_loss, test_acc, true_labels, pred_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n"
   ],
   "id": "bd0ca290f64c4cfd",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T12:03:38.302080057Z",
     "start_time": "2026-02-08T12:01:39.190067378Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "d54032475803e47a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing R(2+1)D model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.77it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0144 Acc: 0.5125\n",
      "Val Loss: 0.7812 Acc: 0.8667\n",
      "Saved best model.\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7828 Acc: 0.8167\n",
      "Val Loss: 0.5593 Acc: 1.0000\n",
      "Saved best model.\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:12<00:00,  2.48it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6009 Acc: 0.9167\n",
      "Val Loss: 0.4153 Acc: 1.0000\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:11<00:00,  2.65it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4525 Acc: 0.9792\n",
      "Val Loss: 0.3079 Acc: 1.0000\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.81it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3721 Acc: 0.9875\n",
      "Val Loss: 0.2374 Acc: 1.0000\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.78it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3586 Acc: 0.9625\n",
      "Val Loss: 0.1918 Acc: 1.0000\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.81it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3081 Acc: 0.9667\n",
      "Val Loss: 0.1535 Acc: 1.0000\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.82it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2664 Acc: 0.9792\n",
      "Val Loss: 0.1534 Acc: 1.0000\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.86it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3195 Acc: 0.9667\n",
      "Val Loss: 0.1520 Acc: 1.0000\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s]\n",
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2956 Acc: 0.9792\n",
      "Val Loss: 0.1460 Acc: 1.0000\n",
      "Best Validation Accuracy: 1.0000\n",
      "\n",
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:00<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9333\n",
      "Test Precision: 0.9444\n",
      "Test Recall: 0.9333\n",
      "Test F1 Score: 0.9327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
